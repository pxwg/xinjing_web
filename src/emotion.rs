use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;
use tracing::{error, info, warn};

#[derive(Debug, Serialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Debug, Deserialize)]
struct OllamaResponse {
    response: String,
}

pub struct EmotionAnalyzer {
    client: Client,
    model_name: String,
    valid_emotions: &'static [&'static str],
}

impl EmotionAnalyzer {
    /// åˆ›å»ºæ–°çš„æƒ…ç»ªåˆ†æå™¨
    pub async fn new() -> Self {
        let analyzer = Self {
            client: Client::new(),
            model_name: "qwen2.5:1.5b".to_string(),
            valid_emotions: &[
                "joy", "anger", "sadness", "fear", "calm", "neutral", "sleep",
            ],
        };

        analyzer.test_connection().await;
        analyzer
    }

    /// åˆ†ææ–‡æœ¬æƒ…ç»ª
    pub async fn analyze(&self, text: &str) -> String {
        let prompt = self.build_emotion_prompt(text);

        match self.send_ollama_request(&prompt).await {
            Ok(response) => self.validate_emotion_response(&response),
            Err(e) => {
                warn!("æƒ…ç»ªåˆ†æå¤±è´¥: {}, ä½¿ç”¨é»˜è®¤æƒ…ç»ª", e);
                "neutral".to_string()
            }
        }
    }

    /// æµ‹è¯•ä¸Ollamaçš„è¿æ¥
    async fn test_connection(&self) {
        match self.send_test_request().await {
            Ok(_) => info!("âœ… Ollama {} æ¨¡å‹è¿æ¥æˆåŠŸ", self.model_name),
            Err(e) => {
                error!("âŒ Ollama è¿æ¥å¤±è´¥: {}", e);
                error!("ğŸ’¡ æç¤º: è¿è¡Œ 'ollama run {}' æ¥å®‰è£…æ¨¡å‹", self.model_name);
            }
        }
    }

    /// å‘é€æµ‹è¯•è¯·æ±‚
    async fn send_test_request(&self) -> Result<(), Box<dyn std::error::Error>> {
        let request = OllamaRequest {
            model: self.model_name.clone(),
            prompt: "æµ‹è¯•".to_string(),
            stream: false,
        };

        let response = self
            .client
            .post("http://127.0.0.1:11434/api/generate")
            .json(&request)
            .timeout(Duration::from_secs(10))
            .send()
            .await?;

        if response.status().is_success() {
            Ok(())
        } else {
            Err(format!("Ollama è¿”å›é”™è¯¯çŠ¶æ€: {}", response.status()).into())
        }
    }

    /// æ„å»ºæƒ…ç»ªåˆ†ææç¤ºè¯
    fn build_emotion_prompt(&self, text: &str) -> String {
        format!(
            "Analyze the sentiment of the following text. ONLY output ONE word, strictly from this list: {:?}. Do NOT output anything else.\n\nText: {}\n\nSentiment:",
            self.valid_emotions, text
        )
    }

    /// å‘é€Ollamaè¯·æ±‚
    async fn send_ollama_request(
        &self,
        prompt: &str,
    ) -> Result<String, Box<dyn std::error::Error>> {
        let request = OllamaRequest {
            model: self.model_name.clone(),
            prompt: prompt.to_string(),
            stream: false,
        };

        let response = self
            .client
            .post("http://127.0.0.1:11434/api/generate")
            .json(&request)
            .timeout(Duration::from_secs(5))
            .send()
            .await?;

        let ollama_resp: OllamaResponse = response.json().await?;
        Ok(ollama_resp.response)
    }

    /// éªŒè¯å¹¶æ¸…ç†æƒ…ç»ªå“åº”
    fn validate_emotion_response(&self, response: &str) -> String {
        let emotion = response.trim().to_lowercase();

        for &valid_emotion in self.valid_emotions {
            if emotion.contains(valid_emotion) {
                return valid_emotion.to_string();
            }
        }

        info!("LLM è¿”å›äº†éé¢„æœŸçš„æƒ…ç»ª: {}, ä½¿ç”¨ neutral", emotion);
        "neutral".to_string()
    }
}
